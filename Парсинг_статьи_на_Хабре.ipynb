{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOB0HjgsjM3lHa8vaxzAnln",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VSKuz/VSKuz/blob/main/%D0%9F%D0%B0%D1%80%D1%81%D0%B8%D0%BD%D0%B3_%D1%81%D1%82%D0%B0%D1%82%D1%8C%D0%B8_%D0%BD%D0%B0_%D0%A5%D0%B0%D0%B1%D1%80%D0%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Учусь парсить, сохраню статью на Хабре"
      ],
      "metadata": {
        "id": "wNs6ayVx6vYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Импорты"
      ],
      "metadata": {
        "id": "x0BZcQ9y61E3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "SvEa1D3CwKkU"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Подгружаю страницу"
      ],
      "metadata": {
        "id": "nLCUp1OX63c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = (\"https://habr.com/ru/articles/568334/\") #Вставляем сайт\n",
        "html = requests.get(url).text # Извлекаем из тела ответа текстовые данные\n",
        "soup = BeautifulSoup(html, 'html5lib') #Тело парсера"
      ],
      "metadata": {
        "id": "vDWL0WgvwQzG"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Обработка данных"
      ],
      "metadata": {
        "id": "vg1LOurx68Pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Text = soup.find_all('div', class_='article-formatted-body article-formatted-body article-formatted-body_version-1') #Переменная с результатом парсинга (в классе 'article...' парсим все блоки 'div')"
      ],
      "metadata": {
        "id": "Uxj6WM0Hxnek"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for info in Text:\n",
        "  print(info.text) #Проверка результата, добавляем .text чтобы убрать всё лишнее"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26sT_nVKyTGt",
        "outputId": "ba5acce7-a069-4ed1-d6d9-550bceb21c63"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "В этой статье я постараюсь понятно рассказать о парсинге данных и его нюансах.\n",
            "\n",
            "\n",
            "\n",
            "Для начала давайте разберемся, что же действительно означает на первый взгляд непонятное слово — парсинг. Прежде всего это процесс сбора данных с последующей их обработкой и анализом. К этому способу прибегают, когда предстоит обработать большой массив информации, с которым сложно справиться вручную. Понятно, что программу, которая занимается парсингом, называют — парсер. С этим вроде бы разобрались.\n",
            "\n",
            "Перейдем к этапам парсинга.\n",
            "\n",
            "\n",
            "Поиск данных\n",
            "Извлечение информации\n",
            "Сохранение данных\n",
            "\n",
            "И так, рассмотрим первый этап парсинга — Поиск данных.\n",
            "Так как нужно парсить что-то полезное и интересное давайте попробуем спарсить информацию с сайта work.ua.\n",
            "Для начала работы, установим 3 библиотеки Python.\n",
            "\n",
            "pip install beautifulsoup4\n",
            "\n",
            "Без цифры 4 вы ставите старый BS3, который работает только под Python(2.х).\n",
            "\n",
            "pip install requests\n",
            "pip install pandas\n",
            "\n",
            "Теперь с помощью этих трех библиотек Python, можно проанализировать нашу веб-страницу.\n",
            "\n",
            "Второй этап парсинга — Извлечение информации.\n",
            "Попробуем получить структуру html-кода нашего сайта.\n",
            "Давайте подключим наши новые библиотеки.\n",
            "\n",
            "import requests\n",
            "from bs4 import BeautifulSoup as bs\n",
            "import pandas as pd\n",
            "\n",
            "И сделаем наш первый get-запрос.\n",
            "\n",
            "URL_TEMPLATE = \"https://www.work.ua/ru/jobs-odesa/?page=2\"\n",
            "r = requests.get(URL_TEMPLATE)\n",
            "print(r.status_code)\n",
            "\n",
            "Статус 200 состояния HTTP — означает, что мы получили положительный ответ от сервера. Прекрасно, теперь получим код странички.\n",
            "\n",
            "print(r.text)\n",
            "\n",
            "Получилось очень много, правда? Давайте попробуем получить названия вакансий на этой страничке. Для этого посмотрим в каком элементе html-кода хранится эта информация.\n",
            "\n",
            "<h2 class=\"add-bottom-sm\"><a href=\"/ru/jobs/3682040/\" title=\"Комірник, вакансия от 5 ноября 2019\">Комірник</a></h2>\n",
            "\n",
            "У нас есть тег h2 с классом «add-bottom-sm», внутри которого содержится тег a. Отлично, теперь получим title элемента a.\n",
            "\n",
            "soup = bs(r.text, \"html.parser\")\n",
            "vacancies_names = soup.find_all('h2', class_='add-bottom-sm')\n",
            "for name in vacancies_names:\n",
            "    print(name.a['title'])\n",
            "\n",
            "Хорошо, мы получили названия вакансий. Давайте спарсим теперь каждую ссылку на вакансию и ее описание. Описание находится в теге p с классом overflow. Ссылка находится все в том же элементе a.\n",
            "\n",
            "<p class=\"overflow\">Some information about vacancy.</p>\n",
            "\n",
            "Получаем такой код.\n",
            "\n",
            "vacancies_info = soup.find_all('p', class_='overflow')\n",
            "for name in vacancies_names:\n",
            "    print('https://www.work.ua'+name.a['href'])\n",
            "for info in vacancies_info:\n",
            "    print(info.text)\n",
            "\n",
            "И последний этап парсинга — Сохранение данных.\n",
            "Давайте соберем всю полученную информацию по страничке и запишем в удобный формат — csv.\n",
            "\n",
            "import requests\n",
            "from bs4 import BeautifulSoup as bs\n",
            "import pandas as pd\n",
            "\n",
            "URL_TEMPLATE = \"https://www.work.ua/ru/jobs-odesa/?page=2\"\n",
            "FILE_NAME = \"test.csv\"\n",
            "\n",
            "\n",
            "def parse(url = URL_TEMPLATE):\n",
            "    result_list = {'href': [], 'title': [], 'about': []}\n",
            "    r = requests.get(url)\n",
            "    soup = bs(r.text, \"html.parser\")\n",
            "    vacancies_names = soup.find_all('h2', class_='add-bottom-sm')\n",
            "    vacancies_info = soup.find_all('p', class_='overflow')\n",
            "    for name in vacancies_names:\n",
            "        result_list['href'].append('https://www.work.ua'+name.a['href'])\n",
            "        result_list['title'].append(name.a['title'])\n",
            "    for info in vacancies_info:\n",
            "        result_list['about'].append(info.text)\n",
            "    return result_list\n",
            "\n",
            "\n",
            "df = pd.DataFrame(data=parse())\n",
            "df.to_csv(FILE_NAME)\n",
            "\n",
            "\n",
            "После запуска появится файл test.csv — с результатами поиска.\n",
            "\n",
            "«Кто владеет информацией, тот владеет миром» (Н. Ротшильд).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Сохранение"
      ],
      "metadata": {
        "id": "fISUaiSp6qFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data = info.text #Сохраняем фильтрованную инфу в переменную\n",
        "type(Data) #проверяю формат результата"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxgBNZJR7fSC",
        "outputId": "e78205c1-d258-465a-a4ca-50eba37abcd4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FILE_NAME = \"test.txt\"\n",
        "with open(FILE_NAME, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(Data) #Так как наш результат - простой текст, сохраняем его без лишнего гемора"
      ],
      "metadata": {
        "id": "O99QK8I96pI_"
      },
      "execution_count": 47,
      "outputs": []
    }
  ]
}